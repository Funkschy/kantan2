import ":std/vec";
import ":std/dbg";

import ":ir/ir";
import ":ir/const";
import ":ir/rvalue";
import ":ir/memory";

import ":util";
import ":types/types" as ty;

import "vm";
import "inst" as _;

type IRCompiler struct {
    current_function: *ir.Function,
    // the offset from the base pointer of all local variables of the current_function
    local_offsets: vec.Vec, // vec.Vec[u64]
    // the location of the first instruction in each bb of the current function
    // the index here is the bb id and the value is the index inside of code
    // this vec is reset when we enter a new function
    bb_locations: vec.Vec, // vec.Vec[u64]
    code: vec.Vec // vec.Vec[u8]
}

def compiler(): IRCompiler {
    return IRCompiler {
        current_function: null,
        local_offsets: vec.create(sizeof u64),
        bb_locations: vec.create(sizeof u64),
        code: vec.create(sizeof u8)
    };
}

def (c: *IRCompiler) free() {
    c.local_offsets.free();
    c.bb_locations.free();
    c.code.free();
}

def (c: *IRCompiler) width_bits(location: *memory.Location): usize {
    return c.typeof(location).width.bits() as usize;
}

def (c: *IRCompiler) typeof(location: *memory.Location): *ty.Type {
    return c.current_function.location_type(location, true);
}

def (c: *IRCompiler) width_bits_op(op: *rvalue.Operand): usize {
    return c.typeof_op(op).width.bits() as usize;
}

def (c: *IRCompiler) typeof_op(op: *rvalue.Operand): *ty.Type {
    if op.kind == rvalue.OperandKind.Copy {
        return c.typeof(&op.data.copy);
    } else {
        return op.data.constant.ty;
    }
}

def (c: *IRCompiler) fill_local_offsets(): u64 {
    c.local_offsets.clear();

    let offset: u64 = 0;
    for let l: u64 = 0; l < c.current_function.body.locals.len as u64; l += 1 {
        let location = memory.local(l as u32 + 1, false).as_location();
        let ty = c.current_function.location_type(&location, false);
        // this was a temporary local
        if ty == null {
            continue;
        }

        if ty.is_unsized() {
            // there are actually locals with type void in the mir, since every expression
            // (calls to void functions in this case) must be assigned to a variable
            c.local_offsets.push(&offset as *void);
            continue;
        }

        let align = ty.align.bytes();
        let width = ty.width.bytes();
        offset = (offset + align - 1) & -align;

        c.local_offsets.push(&offset as *void);

        offset += width;
    }

    return util.next_multiple_of_8(offset);
}

def (c: *IRCompiler) local_offset(local: memory.Local): u64 {
    return *(c.local_offsets.get_ptr(local.idx as usize - 1) as *u64);
}

def (c: *IRCompiler) emit(i: Inst) {
    let byte = i as i32 as i8;
    c.code.push(&byte as *void);
}

def (c: *IRCompiler) emit_with_op(i: Inst, operand: u64, operand_bits: usize) {
    c.emit(i);

    let n = 1;
    if operand_bits == 16 {
        n = 2;
    } else if operand_bits == 32 {
        n = 4;
    } else if operand_bits == 64 {
        n = 8;
    }

    for let i = 0; i < n; i += 1 {
        c.code.push(&operand as *void + i);
    }
}

def (c: *IRCompiler) load_location_address(location: *memory.Location) {
    dbg.assert(location.kind == memory.LocationKind.Local, "globals are not implemented");

    let width = c.width_bits(location);
    let local = location.data.local;

    c.emit_with_op(Inst.LocalPtr, c.local_offset(local), 32);

    let proj_node = location.projection_head;
    while proj_node != null {
        let proj = proj_node.value;
        if proj.kind == memory.ProjectionKind.Deref {
            c.emit(with_size(Inst.Load8, width));
        } else {
            dbg.not_implemented();
        }

        proj_node = proj_node.next;
    }
}

def (c: *IRCompiler) write_to_location(location: *memory.Location) {
    if c.typeof(location).is_unsized() || location.is_temp() {
        // TODO FIXME: i think in case of a temp location we actually need to pop the value
        return;
    }

    dbg.assert(location.kind == memory.LocationKind.Local, "globals are not implemented");
    let local = location.data.local;

    // if this has projections, we first need the address of the memory location
    c.load_location_address(location);
    c.emit(with_size(Inst.Store8, c.width_bits(location)));
}

def (c: *IRCompiler) load_location(location: *memory.Location) {
    if c.typeof(location).is_unsized() || location.is_temp() {
        return;
    }

    c.load_location_address(location);
    c.emit(with_size(Inst.Load8, c.width_bits(location)));
}

def (c: *IRCompiler) load_constant(constant: *const.Constant) {
    let size = constant.ty.width.bits() as usize;

    if constant.kind == const.ConstantKind.Nothing {
        return;
    } else if constant.kind == const.ConstantKind.Null {
        c.emit_with_op(with_size(Inst.ConstI8, size), 0, size);
        return;
    } else if constant.kind == const.ConstantKind.Undefined {
        // TODO: push each field separately for structs
        c.emit_with_op(with_size(Inst.ConstI8, size), 0, size);
        return;
    } else if constant.kind == const.ConstantKind.Char {
        // TODO
    } else if constant.kind == const.ConstantKind.Int {
        let value = constant.data.int;
        c.emit_with_op(with_size(Inst.ConstI8, size), value, size);
        return;
    } else if constant.kind == const.ConstantKind.Float {
        // TODO
    } else if constant.kind == const.ConstantKind.String {
        // TODO
    } else if constant.kind == const.ConstantKind.Bool {
        let value = constant.data.boolean as u64;
        c.emit_with_op(Inst.ConstI8, value, 8);
        return;
    } else if constant.kind == const.ConstantKind.Function {
        // TODO
    }

    dbg.not_implemented();
}

def (c: *IRCompiler) load_operand(operand: *rvalue.Operand) {
    if operand.kind == rvalue.OperandKind.Constant {
        c.load_constant(&operand.data.constant);
    } else {
        c.load_location(&operand.data.copy);
    }
}

def (c: *IRCompiler) load_value_from_expr(expr: *rvalue.Expression) {
    if expr.kind == rvalue.ExpressionKind.Use {
        c.load_operand(&expr.data.use);
        return;
    } else if expr.kind == rvalue.ExpressionKind.Ref {
        c.load_location_address(&expr.data.ref);
        return;
    } else if expr.kind == rvalue.ExpressionKind.Cast {
        // TODO
    } else if expr.kind == rvalue.ExpressionKind.Unary {
        // TODO
    } else if expr.kind == rvalue.ExpressionKind.Binary {
        let binary = &expr.data.binary;
        c.load_operand(&binary.left);
        c.load_operand(&binary.right);

        let left_ty = c.typeof_op(&binary.left);
        if binary.kind == rvalue.BinaryKind.AddScalar || binary.kind == rvalue.BinaryKind.SubScalar {
            // this assumes, that the left part of the expression is always the pointer
            // that constraint should be enforced by the ir compiler
            let ptr_ty = left_ty;
            dbg.assert(ptr_ty.kind == ty.TypeKind.Ptr, "left side of AddScalar should be ptr");

            // multiply the scalar with the pointee size
            let pointee_type = ptr_ty.inner_type();
            let pointee_size: u64 = 1;
            if !pointee_type.is_unsized() {
                pointee_size = pointee_type.width.bytes();
            }

            c.emit_with_op(Inst.ConstI64, pointee_size, 64);
            c.emit(Inst.IMul);

            if binary.kind == rvalue.BinaryKind.AddScalar {
                c.emit(Inst.IAdd);
            } else {
                c.emit(Inst.ISub);
            }
        } else if left_ty.is_integer() {
            let offset = binary.kind as i32;
            let inst = Inst.IAdd as i32 + offset;
            c.emit(*(&inst as *Inst));
        } else {
            dbg.not_implemented();
        }

        return;
    }

    dbg.not_implemented();
}

def (c: *IRCompiler) compile_terminator(terminator: *ir.Terminator) {
    if terminator.kind == ir.TerminatorKind.Jmp {
        // in this stage, we put the bb id as a placeholder. After compiling the function, those
        // placeholders are replaced with the real instruction address
        c.emit_with_op(Inst.Jmp, terminator.data.jmp as u64, 64);
    } else if terminator.kind == ir.TerminatorKind.SwitchInt {
        let switch_int = &terminator.data.switch_int;

        // TODO: this can be a lot more optimized for normal if-else statements

        c.load_operand(&switch_int.condition);
        for let case = switch_int.cases; case != null; case = case.next {
            // if there are n cases, we need to check the condition n times, so we need to duplicate
            // it n-1 times. This way the last condition pops the condition from the stack
            if case.next != null {
                c.emit(Inst.Dup);
            }

            // compare value with the condition, which is already on the stack
            c.emit_with_op(Inst.ConstI64, case.value, 64);
            c.emit(Inst.EQ);

            // in this stage, we put the bb id as a placeholder. After compiling the function, those
            // placeholders are replaced with the real instruction address
            c.emit_with_op(Inst.Jif, case.target as u64, 64);
        }
    }
}

def (c: *IRCompiler) compile_function(f: *ir.Function) {
    c.current_function = f;
    c.bb_locations.clear();
    c.bb_locations.reserve(f.num_bbs());

    let operand_stack_start = c.fill_local_offsets() as u32 as u64; // only use 4 bytes
    c.emit_with_op(Inst.AllocLocals, operand_stack_start, 32);

    for let b: usize = 0; b < f.num_bbs(); b += 1 {
        let bb = f.bb_at(b);
        // push the start address of this basic block
        c.bb_locations.push(&c.code.len as *void);

        for let s: usize = 0; s < bb.num_statements(); s += 1 {
            let stmt = bb.statement_at(s);

            if stmt.kind == ir.StatementKind.Assign {
                let assign = &stmt.data.assign;
                c.load_value_from_expr(&assign.value);

                dbg.assert(assign.location.kind == memory.LocationKind.Local, "globals not implemented");
                if !c.typeof(&assign.location).is_unsized() {
                    c.write_to_location(&assign.location);
                }
            }
        }

        c.compile_terminator(&bb.terminator);
    }

    // fix jump placeholders
    let program = c.code.get_ptr(0) as *u8;
    let program_len = c.code.len;

    for let i: usize = 0; i < c.code.len; {
        let instruction = *(program + i) as i32;
        let instruction = *(&instruction as *Inst);

        if instruction >= Inst.Jmp && instruction <= Inst.Jif {
            let bb_id_placeholder = util.read_int(program + i + 1, 8);
            let real_address = c.bb_locations.get_ptr(bb_id_placeholder as usize) as *u64;
            c.code.set(i + 1, real_address as *void);
        }

        i += instruction.width_bytes();
    }
}

