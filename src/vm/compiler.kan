import ":std/vec";
import ":std/dbg";
import ":std/map";
import ":std/num";

import ":ir/ir";
import ":ir/const";
import ":ir/rvalue";
import ":ir/memory";

import ":util";
import ":types/types" as ty;

import "vm";
import "inst" as _;

type IRCompiler struct {
    current_function: *ir.Function,
    // the offset from the base pointer of all local variables of the current_function
    local_offsets: vec.Vec, // vec.Vec[u64]
    // the location of the first instruction in each bb of the current function
    // the index here is the bb id and the value is the index inside of code
    // this vec is reset when we enter a new function
    bb_locations: vec.Vec, // vec.Vec[u64]
    code: vec.Vec, // vec.Vec[u8]
    // Since anything smaller than 8 bytes (other than function pointers) is currently inlined in
    // the bytecode, this just holds string constants and function pointers
    constant_pool: vec.Vec, // vec.Vec[u64]
    // a mapping of function name to its offset inside the constant pool
    function_constant_offsets: map.Map // map.Map[str.View, usize]
}

def compiler(): IRCompiler {
    return IRCompiler {
        current_function: null,
        local_offsets: vec.create(sizeof u64),
        bb_locations: vec.create(sizeof u64),
        code: vec.create(sizeof u8),
        constant_pool: vec.create(sizeof u64),
        function_constant_offsets: map.create()
    };
}

def (c: *IRCompiler) free() {
    c.local_offsets.free();
    c.bb_locations.free();
    c.code.free();
    c.constant_pool.free();
    c.function_constant_offsets.free();
}

def (c: *IRCompiler) width_bits(location: *memory.Location): usize {
    return c.typeof(location).width.bits() as usize;
}

def (c: *IRCompiler) typeof(location: *memory.Location): *ty.Type {
    return c.current_function.location_type(location, true);
}

def (c: *IRCompiler) width_bits_op(op: *rvalue.Operand): usize {
    return c.typeof_op(op).width.bits() as usize;
}

def (c: *IRCompiler) typeof_op(op: *rvalue.Operand): *ty.Type {
    if op.kind == rvalue.OperandKind.Copy {
        return c.typeof(&op.data.copy);
    } else {
        return op.data.constant.ty;
    }
}

def (c: *IRCompiler) set_constant(index: usize, constant: u64) {
    c.constant_pool.set(index, &constant as *void);
}

def (c: *IRCompiler) read_64bit_constant(index: usize): u64 {
    return *(c.constant_pool.get_ptr(index) as *u64);
}

def (c: *IRCompiler) fill_local_offsets(): u64 {
    c.local_offsets.clear();

    let offset: u64 = 0;
    for let l: u64 = 0; l < c.current_function.body.locals.len as u64; l += 1 {
        let location = memory.local(l as u32 + 1, false).as_location();
        let ty = c.current_function.location_type(&location, false);

        // this was a temporary local or a void local
        if ty == null || ty.is_unsized() {
            // there are actually locals with type void in the mir, since every expression
            // (calls to void functions in this case) must be assigned to a variable
            c.local_offsets.push(&offset as *void);
            continue;
        }

        let align = ty.align.bytes();
        let width = ty.width.bytes();
        offset = (offset + align - 1) & -align;

        c.local_offsets.push(&offset as *void);

        offset += width;
    }

    return util.next_multiple_of_8(offset);
}

def (c: *IRCompiler) local_offset(local: memory.Local): u64 {
    return *(c.local_offsets.get_ptr(local.idx as usize - 1) as *u64);
}

def (c: *IRCompiler) emit(i: Inst) {
    let byte = i as i32 as i8;
    c.code.push(&byte as *void);
}

def (c: *IRCompiler) emit_with_op(i: Inst, operand: u64, operand_bits: usize) {
    c.emit(i);

    let n = 1;
    if operand_bits == 16 {
        n = 2;
    } else if operand_bits == 32 {
        n = 4;
    } else if operand_bits == 64 {
        n = 8;
    }

    for let i = 0; i < n; i += 1 {
        c.code.push(&operand as *void + i);
    }
}

def (c: *IRCompiler) load_location_address(location: *memory.Location) {
    dbg.assert(location.kind == memory.LocationKind.Local, "globals are not implemented");

    let width = c.width_bits(location);
    let local = location.data.local;

    c.emit_with_op(Inst.LocalPtr, c.local_offset(local), 32);

    let proj_node = location.projection_head;
    while proj_node != null {
        let proj = proj_node.value;
        if proj.kind == memory.ProjectionKind.Deref {
            c.emit(with_size(Inst.Load8, width));
        } else {
            dbg.not_implemented();
        }

        proj_node = proj_node.next;
    }
}

def (c: *IRCompiler) write_to_location(location: *memory.Location) {
    if c.typeof(location).is_unsized() || location.is_temp() {
        return;
    }

    dbg.assert(location.kind == memory.LocationKind.Local, "globals are not implemented");
    let local = location.data.local;

    // if this has projections, we first need the address of the memory location
    c.load_location_address(location);
    c.emit(with_size(Inst.Store8, c.width_bits(location)));
}

def (c: *IRCompiler) load_location(location: *memory.Location) {
    if c.typeof(location).is_unsized() {
        return;
    }

    if location.is_temp() {
        return;
    }

    c.load_location_address(location);
    c.emit(with_size(Inst.Load8, c.width_bits(location)));
}

def (c: *IRCompiler) load_constant(constant: *const.Constant) {
    if constant.kind == const.ConstantKind.Nothing {
        return;
    } else if constant.kind == const.ConstantKind.Null {
        let size = constant.ty.width.bits() as usize;
        c.emit_with_op(with_size(Inst.ConstI8, size), 0, size);
        return;
    } else if constant.kind == const.ConstantKind.Undefined {
        let size = constant.ty.width.bits() as usize;
        // TODO: push each field separately for structs
        c.emit_with_op(with_size(Inst.ConstI8, size), 0, size);
        return;
    } else if constant.kind == const.ConstantKind.Char {
        // TODO
    } else if constant.kind == const.ConstantKind.Int {
        let size = constant.ty.width.bits() as usize;
        let value = constant.data.int;
        c.emit_with_op(with_size(Inst.ConstI8, size), value, size);
        return;
    } else if constant.kind == const.ConstantKind.Float {
        // TODO
    } else if constant.kind == const.ConstantKind.String {
        // TODO
    } else if constant.kind == const.ConstantKind.Bool {
        let value = constant.data.boolean as u64;
        c.emit_with_op(Inst.ConstI8, value, 8);
        return;
    } else if constant.kind == const.ConstantKind.Function {
        let name = constant.data.function.data.signature.name.as_view();
        let const_offset = num.ptr_to_int(c.function_constant_offsets.get(map.key(name)));
        c.emit_with_op(Inst.LoadConst, const_offset as u64, 32);
        return;
    }

    dbg.not_implemented();
}

def (c: *IRCompiler) load_operand(operand: *rvalue.Operand) {
    if operand.kind == rvalue.OperandKind.Constant {
        c.load_constant(&operand.data.constant);
    } else {
        c.load_location(&operand.data.copy);
    }
}

def (c: *IRCompiler) load_value_from_expr(expr: *rvalue.Expression) {
    if expr.kind == rvalue.ExpressionKind.Use {
        c.load_operand(&expr.data.use);
        return;
    }

    if expr.kind == rvalue.ExpressionKind.Ref {
        c.load_location_address(&expr.data.ref);
        return;
    }

    if expr.kind == rvalue.ExpressionKind.Cast {
        // TODO
    }

    if expr.kind == rvalue.ExpressionKind.Unary {
        let unary = &expr.data.unary;
        let op = &unary.operand;

        c.load_operand(op);

        if unary.kind == rvalue.UnaryKind.Not && c.typeof_op(op).kind == ty.TypeKind.Bool {
            c.emit(Inst.BoolNot);
            return;
        }

        let offset = unary.kind as i32;
        let inst = Inst.Not as i32 + offset;
        c.emit(*(&inst as *Inst));

        return;
    }

    if expr.kind == rvalue.ExpressionKind.Binary {
        let binary = &expr.data.binary;
        let l = &binary.left;
        let r = &binary.right;

        c.load_operand(&binary.left);
        c.load_operand(&binary.right);

        let l_is_temp = l.kind == rvalue.OperandKind.Copy && l.data.copy.is_temp();
        let r_is_temp = r.kind == rvalue.OperandKind.Copy && r.data.copy.is_temp();
        if !l_is_temp && r_is_temp {
            // if the rhs of a binary operation is a temporary read, that means it's on the stack
            // below the lhs, because the actual expression was already evaluated
            // e.g: 4 < 2 + 1 looks like this in the ir:
            // {
            //   'op': 'assign',
            //   'dest': 't1',
            //   'value': {
            //     'op': 'add',
            //     'left': { 'const': 2 },
            //     'right': { 'const': 1 },
            //   }
            // },
            // {
            //   'op': '<',
            //   'left': { 'const': 4 }
            //   'right': { 'load': t1 }
            // }
            //
            // in that case the 2+1 will 'written' into a temp var, that means the evaluation order is:
            // push 2
            // push 1
            // add
            // 'emitted pop into temp_1'
            // push 4
            // 'emitted load from temp_1'
            // lt
            // so the stack before the 'lt' would be:
            // 3
            // 4
            // which is wrong
            // TODO: instead of emitting a swap, we could just eval the right operand first
            //  but this is easier to debug, since you can see it in the bytecode

            c.emit(Inst.Swap);
        }

        let left_ty = c.typeof_op(&binary.left);
        if binary.kind == rvalue.BinaryKind.AddScalar || binary.kind == rvalue.BinaryKind.SubScalar {
            // this assumes, that the left part of the expression is always the pointer
            // that constraint should be enforced by the ir compiler
            let ptr_ty = left_ty;
            dbg.assert(ptr_ty.kind == ty.TypeKind.Ptr, "left side of AddScalar should be ptr");

            // multiply the scalar with the pointee size
            let pointee_type = ptr_ty.inner_type();
            let pointee_size: u64 = 1;
            if !pointee_type.is_unsized() {
                pointee_size = pointee_type.width.bytes();
            }

            c.emit_with_op(Inst.ConstI64, pointee_size, 64);
            c.emit(Inst.IMul);

            if binary.kind == rvalue.BinaryKind.AddScalar {
                c.emit(Inst.IAdd);
            } else {
                c.emit(Inst.ISub);
            }
        }

        let offset = binary.kind as i32;
        let inst = Inst.IAdd as i32 + offset;
        c.emit(*(&inst as *Inst));

        return;
    }

    dbg.not_implemented();
}

def (c: *IRCompiler) compile_terminator(terminator: *ir.Terminator) {
    if terminator.kind == ir.TerminatorKind.Jmp {
        // in this stage, we put the bb id as a placeholder. After compiling the function, those
        // placeholders are replaced with the real instruction address
        c.emit_with_op(Inst.Jmp, terminator.data.jmp as u64, 64);
        return;
    } else if terminator.kind == ir.TerminatorKind.SwitchInt {
        let switch_int = &terminator.data.switch_int;

        dbg.assert(
            c.typeof_op(&switch_int.condition).kind == ty.TypeKind.Bool,
            "switch int is only implemented for if-else right now"
        );
        dbg.assert(switch_int.cases != null, "no cases for if");
        dbg.assert(switch_int.cases.next != null, "no else case for if");

        let true_case = switch_int.cases;
        let false_case = true_case.next;
        dbg.assert(false_case.next == null, "more than 2 cases for boolean if");

        if true_case.value != 1 {
            let temp = true_case;
            true_case = false_case;
            false_case = temp;
        }

        c.load_operand(&switch_int.condition);
        // in this stage, we put the bb id as a placeholder. After compiling the function, those
        // placeholders are replaced with the real instruction address
        c.emit_with_op(Inst.Jif, true_case.target as u64, 64);
        c.emit_with_op(Inst.Jmp, false_case.target as u64, 64);
        return;
    } else if terminator.kind == ir.TerminatorKind.Call {
        let call = &terminator.data.call;

        // TODO: push args
        c.load_operand(&call.callee);
        c.emit_with_op(Inst.Call, call.nargs as u64, 32);
        return;
    } else if terminator.kind == ir.TerminatorKind.Return {
        c.load_operand(&terminator.data.ret);
        c.emit(Inst.Return);
        return;
    } else if terminator.kind == ir.TerminatorKind.Nop {
        return;
    }

    import "io"; io.printf("%d\n", terminator.kind);
    dbg.not_implemented();
}

def (c: *IRCompiler) compile_program(functions: map.Map) {
    // reserve space for the function offsets in the constant pool
    c.constant_pool.reserve(functions.len * sizeof *void);

    c.function_constant_offsets.free();
    c.function_constant_offsets = map.with_cap(functions.len);

    // placeholder for jump to main
    c.emit_with_op(Inst.ConstI32, 0, 32);
    // TODO: pass argc and argv to main
    c.emit_with_op(Inst.Call, 0, 32);
    c.emit(Inst.Halt);

    // init function offsets
    let const_offset = c.constant_pool.len;
    let iter = functions.iter();
    for let item = iter.next(); item != null; item = iter.next() {
        let function = item.value as *ir.Function;
        let name = function.decl.original_name.as_view();
        c.function_constant_offsets.insert(map.key(name), num.int_to_ptr(const_offset));
        const_offset += 1;
    }

    let iter = functions.iter();
    for let item = iter.next(); item != null; item = iter.next() {
        let function = item.value as *ir.Function;
        c.compile_function(function);
    }

    // fix mains address in first instruction
    let main_offset_idx = num.ptr_to_int(c.function_constant_offsets.get(map.create_key(4, "main")));
    let main_offset = c.read_64bit_constant(main_offset_idx);
    let actual_len = c.code.len;
    c.code.len = 0;
    c.emit_with_op(Inst.ConstI32, main_offset as u64, 32);
    c.code.len = actual_len;
}

def (c: *IRCompiler) compile_function(f: *ir.Function) {
    c.current_function = f;
    c.bb_locations.clear();
    c.bb_locations.reserve(f.num_bbs());
    let start = c.code.len;

    let name = f.decl.original_name.as_view();
    let function_offset_constant_idx = num.ptr_to_int(c.function_constant_offsets.get(map.key(name)));
    c.set_constant(function_offset_constant_idx, c.code.len as u64);

    let operand_stack_start = c.fill_local_offsets() as u32 as u64; // only use 4 bytes
    c.emit_with_op(Inst.EnterFunction, operand_stack_start, 32);

    for let b: usize = 0; b < f.num_bbs(); b += 1 {
        let bb = f.bb_at(b);
        // push the start address of this basic block
        c.bb_locations.push(&c.code.len as *void);

        for let s: usize = 0; s < bb.num_statements(); s += 1 {
            let stmt = bb.statement_at(s);

            if stmt.kind == ir.StatementKind.Assign {
                let assign = &stmt.data.assign;
                c.load_value_from_expr(&assign.value);

                dbg.assert(assign.location.kind == memory.LocationKind.Local, "globals not implemented");
                if !c.typeof(&assign.location).is_unsized() {
                    c.write_to_location(&assign.location);
                }
            }
        }

        c.compile_terminator(&bb.terminator);

        if bb.num_statements() == 0 {
            c.emit(Inst.Nop); // TODO: is this needed?
        }
    }

    // fix jump placeholders
    let program = c.code.get_ptr(0) as *u8;
    let program_len = c.code.len;

    for let i: usize = start; i < program_len; {
        let instruction = *(program + i) as i32;
        let instruction = *(&instruction as *Inst);

        if instruction >= Inst.Jmp && instruction <= Inst.Jif {
            let bb_id_placeholder = util.read_int(program + i + 1, 8);
            let real_address = c.bb_locations.get_ptr(bb_id_placeholder as usize) as *u64;
            c.code.set(i + 1, real_address as *void);
        }

        i += instruction.width_bytes();
    }
}

