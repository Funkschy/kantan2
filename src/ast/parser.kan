import "../std/dbg";
import "../std/str";
import "../std/vec";

import "../cli/report" as _report; // rename this, so I don't use it instead of Parser.report()
import "../error" as e;
import "../memory/arena";
import "../util" as _;

import "../source/span";
import "../source/ident";

import "tyid";
import "item";
import "stmt" as _;
import "expr" as _;
import "signature" as sig;

import "lexer" as l;
import "token" as tok;

type Parser struct {
    panic_mode: bool,
    error_count: usize,
    lexer: l.Lexer,
    ast_arena: *arena.TypedArena,
    ty_arena: *arena.TypedArena,
    expr_arena: *arena.TypedArena
}

def create(
    lexer: l.Lexer,
    ast_arena: *arena.TypedArena,
    ty_arena: *arena.TypedArena,
    expr_arena: *arena.TypedArena
): Parser {
    dbg.assert(ast_arena.elem_size == sizeof item.Item, "wrong ast arena size");
    dbg.assert(expr_arena.elem_size == sizeof Expr, "wrong expr arena size");
    dbg.assert(ty_arena.elem_size == sizeof tyid.Type, "wrong ty arena size");

    return Parser {
        panic_mode: false,
        error_count: 0,
        lexer: lexer,
        ast_arena: ast_arena,
        ty_arena: ty_arena,
        expr_arena: expr_arena
    };
}

def (p: *Parser) alloc_item(): *item.Item {
    return p.ast_arena.alloc() as *item.Item;
}

def (p: *Parser) alloc_tyid(): *tyid.Type {
    return p.ty_arena.alloc() as *tyid.Type;
}

def (p: *Parser) alloc_expr(): *Expr {
    return p.expr_arena.alloc() as *Expr;
}

def (p: *Parser) peek(): tok.Token {
    return p.lexer.peek();
}

def (p: *Parser) advance(): tok.Token {
    let current = p.lexer.next_token();

    while current.ty.is_err() {
        let error = e.simple_error(e.ErrorKind.CouldNotScanToken, current.span);
        let note = e.simple_note(current.ty.as_view());
        error.add_note(&note);
        p.report(&error);

        current = p.lexer.next_token();
    }

    return current;
}

def (p: *Parser) check(ty: tok.TokenType): bool {
    return p.peek().ty == ty;
}

def (p: *Parser) is_at_end(): bool {
    return p.check(tok.TokenType.EOF);
}

// consumes a token of the specified type, or does nothing
// returns true, if a token was consumed
def (p: *Parser) consume_optional(ty: tok.TokenType): bool {
    if p.check(ty) {
        p.advance();
        return true;
    }
    return false;
}

// see consume_optional and consume_tok
def (p: *Parser) consume_optional_tok(ty: tok.TokenType, out: *tok.Token): bool {
    if p.check(ty) {
        *out = p.advance();
        return true;
    }
    return false;
}

// this will first check, if the next token the lexer would produce, will be of the expected
// type, if it is, it will set out to the consumed token and return Result.OK
// if the token type differs, it will report a parse error and return Result.Error.
// In the latter case, the value of out is undefined
// Any error tokens will be skipped and reported. If the expected token follows after some
// amount of error tokens, this will succeed, but report the first error that was encountered during
// skipping.
def (p: *Parser) consume_tok(expected: tok.TokenType, out: *tok.Token): Result {
    if p.check(expected) {
        *out = p.advance();
        return Result.OK;
    }

    // skip all error tokens so that we don't report 2 errors for a single wrong token
    // (UnknownToken + ExpectedButGot)
    if p.peek().ty.is_err() {
        let next_after_errors = p.advance();
        if next_after_errors.ty == expected {
            *out = next_after_errors;
            return Result.OK;
        }
    }

    // if we get here, report the consume error (ExpectedButGot)
    p.report_expected_but_got(expected.as_view());
    return Result.Error;
}

// same as consume_tok, but always discards the consumed token
def (p: *Parser) consume(expected: tok.TokenType): Result {
    let out: tok.Token = undefined;
    return p.consume_tok(expected, &out);
}

def (p: *Parser) consume_ident(out: *ident.Ident): Result {
    let name_token: tok.Token = undefined;
    if p.consume_tok(tok.TokenType.Ident, &name_token).is_error() {
        return Result.Error;
    }

    *out = ident.from_span(name_token.span);
    return Result.OK;
}

// consume a type identifier into the previously allocated memory
// the caller has to ensure, that tyid.Type.free is called in the case of an error.
// However, this should be handled by the arena
def (p: *Parser) consume_tyid(memory: *tyid.Type): Result {
    // if the type starts with '*', it's a pointer. The inner type will be parsed recursively
    let star: tok.Token = undefined;
    if p.consume_optional_tok(tok.TokenType.Star, &star) {
        let inner = p.alloc_tyid();
        tyid.init_pointer(memory, inner);
        if p.consume_tyid(inner).is_error() {
            return Result.Error;
        }

        memory.span = star.span.merge(inner.span);
        return Result.OK;
    }

    tyid.init_path(memory);
    let path = &memory.data.path;

    // a segment is just an identifier
    // a Path can consist of an arbitrary amount of Segments, which are connected by dots
    // e.g. tyid.Type.free has 3 segments
    let segment: ident.Ident = undefined;
    if p.consume_ident(&segment).is_error() {
        return Result.Error;
    }

    memory.span = segment.span;
    path.push_segment(segment);

    while p.check(tok.TokenType.Dot) {
        p.advance();
        if p.consume_ident(&segment).is_error() {
            return Result.Error;
        }
        memory.span = memory.span.merge(segment.span);
        path.push_segment(segment);
    }

    return Result.OK;
}

def (p: *Parser) report_expected_but_got(expected: str.View) {
    let actual = p.peek().ty.as_view();
    let error = e.string_tuple(e.ErrorKind.ExpectedButGot, p.peek().span, expected, actual);
    p.report(&error);
}

def (p: *Parser) report_simple(kind: e.ErrorKind, span: span.Span) {
    let error = e.simple_error(kind, span);
    p.report(&error);
}

// report an error. If the parser is currently in panic mode, the error will be silently
// discarded
def (p: *Parser) report(error: *e.Error) {
    if p.panic_mode {
        return;
    }

    p.panic_mode = true;
    p.error_count += 1;
    _report.print_error(error);
}

// try to synchronize the compiler. This will skip tokens until we are at the beginning of a new
// stmt or EOF
// it will return the span of the last consume token
def (p: *Parser) sync(): span.Span {
    p.panic_mode = false;

    let previous = p.advance();
    while !p.check(tok.TokenType.EOF) {
        if previous.ty == tok.TokenType.Semi {
            return previous.span;
        }

        let current = p.peek();
        if current.ty == tok.TokenType.Import {
            return previous.span;
        }

        previous = p.advance();
    }

    return previous.span;
}

// Basically a Result type for import parsing, since ImportDeclarations aren't Items, we cannot
// put them inside the item arena. So we just store them in a separate Vec by value
type ParsedImport struct {
    // since we return the parsed import by value, we have to somehow mark the last import
    is_present: bool,
    // we want to report every error, so we have to treat a missing import different than an
    // erroneous one
    was_error: bool,
    import_decl: item.ImportDeclaration
}

def empty_import(): ParsedImport {
    return ParsedImport { is_present: false, was_error: false, import_decl: undefined };
}

def error_import(): ParsedImport {
    return ParsedImport { is_present: false, was_error: true, import_decl: undefined };
}

// since all imports have to be declared at the top of the file, they can be parsed separate
// from all the other items. This is important for dependency resolution between files, to avoid
// parsing files, that aren't even included (e.g. the stdlib)
def (p: *Parser) parse_next_import(): ParsedImport {
    if !p.check(tok.TokenType.Import) {
        return empty_import();
    }

    // consume 'import'
    p.advance();

    let import_path: tok.Token = undefined;
    if p.consume_tok(tok.TokenType.String, &import_path).is_error() {
        p.sync();
        return error_import();
    }

    if p.consume(tok.TokenType.Semi).is_error() {
        p.sync();
        return error_import();
    }

    let decl = item.ImportDeclaration {
        path: ident.from_span(import_path.span),
        alias: ident.empty()
    };

    return ParsedImport { is_present: true, was_error: false, import_decl: decl };
}

def (p: *Parser) parse_item(): *item.Item {
    // in this model, the caller (this function) allocates the memory neccessary to store an item
    let item: *item.Item = null;

    if p.check(tok.TokenType.Def) {
        if p.parse_func_def(item = p.alloc_item()).is_ok() {
            return item;
        }

        return null;
    }

    p.report_expected_but_got(str.view_from("def"));
    p.sync();
    return null;
}

def (p: *Parser) statement(memory: *Stmt): Result {
    let error_count = p.error_count;
    let start = p.peek().span;

    if p.parse_stmt(memory).is_error() {
        let current = p.peek().span;
        if p.panic_mode {
            current = p.sync();
        }

        // an error occured, but we haven't reported anything yet, so we just use the generic
        // "failed to parse statement" error
        if p.error_count == error_count {
            p.report_simple(e.ErrorKind.CouldNotParseStmt, start.merge(current));
        }

        return Result.Error;
    }

    return Result.OK;
}

def (p: *Parser) parse_stmt(memory: *Stmt): Result {
    // CompoundStatements
    if p.check(tok.TokenType.LBrace) {
        memory.init_block();
        if p.parse_block(memory).is_error() {
            return Result.Error;
        }
        return Result.OK;
    }

    // all statements after this have to be followed by a semicolon (SimpleStatement)
    if p.check(tok.TokenType.Let) {
        memory.init_var_decl();
        if p.parse_let_binding(memory).is_error() {
            return Result.Error;
        }
    } else {
        memory.init_expr();
        if p.expression(&memory.data.expr).is_error() {
            return Result.Error;
        }
        // Parser.expression cannot initialize the span, since it only get the expr memory
        memory.span = memory.data.expr.span;
    }

    if p.consume(tok.TokenType.Semi).is_error() {
        return Result.Error;
    }

    return Result.OK;
}

// parse a single function definition and store it in 'memory'
def (p: *Parser) parse_func_def(memory: *item.Item): Result {
    memory.init_func_def();

    let def_token: tok.Token = undefined;
    if p.consume_tok(tok.TokenType.Def, &def_token).is_error() {
        return Result.Error;
    }

    let name: ident.Ident = undefined;
    if p.consume_ident(&name).is_error() {
        return Result.Error;
    }

    memory.name = name;
    let signature = &memory.data.func_def.sig;

    // the parameters
    let params = &signature.params;
    let end_span = span.empty();
    p.parse_param_list(params, &end_span);

    // the return type must be initialized even if there was no return type, since we might need
    // the span for error messages. In the case of a missing return type declaration, we just
    // use the closing ')' of the param list as the return type span
    let ret = sig.empty_return(end_span);

    // the optional return type
    if p.consume_optional(tok.TokenType.Colon) {
        let ty = p.alloc_tyid();
        if p.consume_tyid(ty).is_error() {
            return Result.Error;
        }
        // if we actually had a return type, we obviously use that instead of the 'ret' above
        ret = sig.return_from_tyid(ty);
    }
    memory.data.func_def.sig.ret = ret;

    let block = &memory.data.func_def.block_stmt;
    if p.parse_block(block).is_error() {
        return Result.Error;
    }

    return Result.OK;
}

// parses the parameter list for a function declaration
// the parsed parameters are pushed into the 'params' vec and the span of the closing ')' is
// returned via the rparen_span parameter
// it does allow for a comma after the last parameter, but not in front of the first
// see grammar.ebnf -> ParameterList for more information
def (p: *Parser) parse_param_list(params: *vec.Vec, rparen_span: *span.Span): Result {
    if p.consume(tok.TokenType.LParen).is_error() {
        return Result.Error;
    }

    let first = true;

    // since we allow for trailing ',', we have to check for the closing ')'
    while !p.check(tok.TokenType.RParen) {
        // there should be no comma infront of the first parameter, so we can just skip this
        if !first {
            // every other parameter should be preceeded by a ','
            if p.consume(tok.TokenType.Comma).is_error() {
                return Result.Error;
            }

            // this allows for a trailing comma after the last parameter
            if p.check(tok.TokenType.RParen) {
                break;
            }
        }
        first = false;

        // a parameter has the form 'name: ty'
        let name: ident.Ident = undefined;
        if p.consume_ident(&name).is_error() {
            return Result.Error;
        }

        if p.consume(tok.TokenType.Colon).is_error() {
            return Result.Error;
        }

        let ty = p.alloc_tyid();
        if p.consume_tyid(ty).is_error() {
            return Result.Error;
        }

        let param = sig.param(name, ty);
        params.push(&param as *void);
    }

    let closing_rparen: tok.Token = undefined;
    if p.consume_tok(tok.TokenType.RParen, &closing_rparen).is_error() {
        return Result.Error;
    }

    *rparen_span = closing_rparen.span;
    return Result.OK;
}

def (p: *Parser) parse_block(memory: *Stmt): Result {
    let left_brace: tok.Token = undefined;
    if p.consume_tok(tok.TokenType.LBrace, &left_brace).is_error() {
        return Result.Error;
    }

    let block = &memory.data.block;
    let result = Result.OK;
    while !(p.is_at_end() || p.check(tok.TokenType.RBrace)) {
        // we reserve memory inside the block and the parse directly into it
        let stmt = block.reserve();
        if p.statement(stmt).is_error() {
            result = Result.Error;
            continue;
        }
        // mark the reserved memory inside the block as initialized
        block.push_reserved();
    }

    let right_brace: tok.Token = undefined;
    if p.consume_tok(tok.TokenType.RBrace, &right_brace).is_error() {
        return Result.Error;
    }

    memory.span = left_brace.span.merge(right_brace.span);
    return result;
}

def (p: *Parser) parse_let_binding(memory: *Stmt): Result {
    let local = &memory.data.local;

    let let_tok: tok.Token = undefined;
    if p.consume_tok(tok.TokenType.Let, &let_tok).is_error() {
        return Result.Error;
    }

    let name: ident.Ident = undefined;
    if p.consume_ident(&name).is_error() {
        return Result.Error;
    }
    local.name = name;

    // a let statement may have an optional type identifier, which can change the type of the expr
    // let i = 0;       // sizeof(i) == 4
    // let i: i64 = 0;  // sizeof(i) == 8
    if p.consume_optional(tok.TokenType.Colon) {
        let ty = p.alloc_tyid();
        if p.consume_tyid(ty).is_error() {
            return Result.Error;
        }

        local.ty = ty;
    }

    if p.consume(tok.TokenType.Eq).is_error() {
        return Result.Error;
    }

    local.value = p.alloc_expr();
    if p.expression(local.value).is_error() {
        return Result.Error;
    }

    return Result.OK;
}

// parse a single expression
def (p: *Parser) expression(memory: *Expr): Result {
    return p.parse_precedence(memory, tok.Precedence.Assign);
}

// parse an expression with a certain start precedence (current). This will consume tokens
// until a token with a lower precedence than 'current' is found
// the resulting expression is stored in 'memory'
def (p: *Parser) parse_precedence(memory: *Expr, current: tok.Precedence): Result {
    if p.parse_prefix(memory).is_error() {
        return Result.Error;
    }

    while p.next_higher_or_eq_precedence(current) {
        // copy the old prefix expression data...
        let prefix = p.alloc_expr();
        *prefix = *memory;
        // ... since we overwrite it here
        if p.parse_infix(memory, prefix).is_error() {
            return Result.Error;
        }
    }

    return Result.OK;
}

def (p: *Parser) next_higher_or_eq_precedence(current: tok.Precedence): bool {
    return p.peek().ty.precedence() >= current;
}

// parse a prefix expression. e.g. a literal or a unary expression
// the parsed expression will be initialized into 'memory'
def (p: *Parser) parse_prefix(memory: *Expr): Result {
    let token = p.advance();

    if token.ty.is_literal() {
        memory.init_lit(token);
        return Result.OK;
    }

    if token.ty == tok.TokenType.Ident {
        memory.init_ident(ident.from_span(token.span));
        return Result.OK;
    }

    if token.ty == tok.TokenType.LParen {
        let start_span = token.span;
        if p.expression(memory).is_error() {
            return Result.Error;
        }
        if p.consume_tok(tok.TokenType.RParen, &token).is_error() {
            return Result.Error;
        }

        memory.span = start_span.merge(token.span);
        return Result.OK;
    }

    // unary deref
    if token.ty == tok.TokenType.Star {
        let right = p.alloc_expr();
        if p.parse_precedence(right, tok.Precedence.Unary).is_error() {
            return Result.Error;
        }

        memory.init_unary(UnaryKind.Deref, token, right);
        return Result.OK;
    }

    p.report_simple(e.ErrorKind.ExpectedExpression, token.span);
    return Result.Error;
}

// parse an infix expression. e.g. a binary expression
// the parsed expression will be initialized into 'memory'
def (p: *Parser) parse_infix(memory: *Expr, left: *Expr): Result {
    let token = p.advance();

    // + - * / %
    if token.ty.is_between(tok.TokenType.Plus, tok.TokenType.Percent) {
        let right = p.alloc_expr();
        // calling parse_precedence with a higher precedence makes the expression left-associative
        // using the same precedence would make it right-associative
        let precedence = token.ty.precedence().next_higher();
        if p.parse_precedence(right, precedence).is_error() {
            return Result.Error;
        }

        let offset = token.ty as i32 - tok.TokenType.Plus as i32;
        memory.init_binary(BinaryKind.Add.with_offset(offset), left, right);
        return Result.OK;
    }

    // TODO: assignment parsing needs tests
    // TODO: check if left is an lvalue
    // assignments
    if token.ty.is_between(tok.TokenType.Eq, tok.TokenType.CaretEq) {
        let right = p.alloc_expr();
        // using the same precedence makes assignments right-associative
        let precedence = token.ty.precedence();
        if p.parse_precedence(right, precedence).is_error() {
            return Result.Error;
        }

        let offset = token.ty as i32 - tok.TokenType.Eq as i32;
        memory.init_assign(AssignKind.Eq.with_offset(offset), left, right);
        return Result.OK;
    }

    p.report_simple(e.ErrorKind.ExpectedOperator, token.span);
    return Result.Error;
}
