import "../std/str";
import "../std/libc";

import "../source/span";

import "token" as _;

type Lexer struct {
    has_peek: bool,
    peeked: Token,
    source: str.View,
    start: *i8,
    current_code_pos: *i8
}

def create(code: str.View): Lexer {
    let span = span.create(code.data, code.data);
    let init_peek = Token { ty: TokenType.UnknownErr, span: span };

    return Lexer {
        has_peek: false,
        peeked: init_peek,
        source: code,
        start: code.data,
        current_code_pos: code.data
    };
}

def (l: *Lexer) current_len(): usize {
    return l.current_code_pos - l.start;
}

def (l: *Lexer) end_pos(): usize {
    return l.source.len - 1;
}

def (l: *Lexer) peek(): Token {
    if l.has_peek {
        return l.peeked;
    }

    l.peeked = l.next_token();
    l.has_peek = true;
    return l.peeked;
}

def (l: *Lexer) next_token(): Token {
    if l.has_peek {
        l.has_peek = false;
        return l.peeked;
    }

    l.skip_whitespace();
    return l.get_next_token();
}

def (l: *Lexer) current_pos(): usize {
    if l.has_peek {
        return (l.peeked.span.start - l.source.data) as usize;
    }

    return (l.current_code_pos - l.source.data) as usize;
}

def (l: *Lexer) current_ptr(): *i8 {
    if l.has_peek {
        return l.peeked.span.start;
    }

    return l.current_code_pos;
}

def (l: *Lexer) current(): i32 {
    let c = 0;
    let read_bytes = l.current_pos();
    read_char(l.current_code_pos, l.source.len - read_bytes, &c);
    return c;
}

def is_num(c: i32): bool {
    return c >= '0' && c <= '9';
}

def is_letter(c: i32): bool {
    return c >= 'a' && c <= 'z'
        || c >= 'A' && c <= 'Z';
}

def (l: *Lexer) check_keyword(start: usize, rest_len: usize, rest: string, ty: TokenType): TokenType {
    if l.current_len() == start + rest_len {
        if libc.memcmp((l.start + start) as *void, rest as *void, rest_len) == 0 {
            return ty;
        }
    }

    return TokenType.Ident;
}

def (l: *Lexer) ident_type(): TokenType {
    let bytes_until = (l.start - l.source.data) as usize;
    let start = 0;
    let len = read_char(l.start, l.source.len - bytes_until, &start);

    if start == 'l' {
        return l.check_keyword(1, 2, "et", TokenType.Let);
    } else if start == 'i' && l.current_len() > 1 {
        let next = 0;
        read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'f' {
            return l.check_keyword(2, 0, "", TokenType.If);
        } else if next == 'm' {
            return l.check_keyword(2, 4, "port", TokenType.Import);
        }
    } else if start == 'd' && l.current_len() > 2 {
        let next = 0;
        len += read_char(l.start + len, l.source.len - bytes_until - len, &next);
        // read third char
        read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'f' {
            if l.current_len() == 3 {
                return l.check_keyword(1, 2, "ef", TokenType.Def);
            } else {
                return l.check_keyword(1, 4, "efer", TokenType.Defer);
            }
        } else if next == 'l' {
            if l.current_len() == 6 {
                return l.check_keyword(1, 5, "elete", TokenType.Delete);
            } else {
                return l.check_keyword(1, 7, "elegate", TokenType.Delegate);
            }
        }
    } else if start == 'e' && l.current_len() > 3 {
        let next = 0;
        len += read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'x' {
            // read third char
            read_char(l.start + len, l.source.len - bytes_until - len, &next);

            if next == 't' {
                return l.check_keyword(2, 4, "tern", TokenType.Extern);
            } else {
                return l.check_keyword(2, 4, "port", TokenType.Export);
            }
        } else if next == 'l' {
            return l.check_keyword(2, 2, "se", TokenType.Else);
        } else if next == 'n' {
            return l.check_keyword(2, 2, "um", TokenType.Enum);
        }
    } else if start == 'n' && l.current_len() > 2 {
        let next = 0;
        read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'u' {
            return l.check_keyword(2, 2, "ll", TokenType.Null);
        } else if next == 'e' {
            return l.check_keyword(2, 1, "w", TokenType.New);
        }
    } else if start == 's' && l.current_len() > 5 {
        let next = 0;
        read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 't' {
            return l.check_keyword(2, 4, "ruct", TokenType.Struct);
        } else if next == 'i' {
            return l.check_keyword(2, 4, "zeof", TokenType.Sizeof);
        }
    } else if start == 'u' && l.current_len() > 4 {
        let next = 0;
        len += read_char(l.start + len, l.source.len - bytes_until - len, &next);
        // read third char
        read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'i' {
            return l.check_keyword(1, 4, "nion", TokenType.Union);
        } else if next == 'd' {
            return l.check_keyword(1, 8, "ndefined", TokenType.Undefined);
        }
    } else if start == 'w' {
        return l.check_keyword(1, 4, "hile", TokenType.While);
    } else if start == 't' {
        return l.check_keyword(1, 3, "ype", TokenType.Type);
    } else if start == 'r' {
        return l.check_keyword(1, 5, "eturn", TokenType.Return);
    } else if start == 'a' {
        return l.check_keyword(1, 1, "s", TokenType.As);
    } else if start == 'f' {
        return l.check_keyword(1, 2, "or", TokenType.For);
    } else if start == 'c' {
        return l.check_keyword(1, 7, "ontinue", TokenType.Continue);
    } else if start == 'b' {
        return l.check_keyword(1, 4, "reak", TokenType.Break);
    }

    return TokenType.Ident;
}

def (l: *Lexer) lex_ident(out: *Token): bool {
    let contains_letter = false;
    while is_letter(l.current()) || is_num(l.current()) || l.current() == '_' {
        contains_letter |= is_letter(l.current());
        l.advance();
    }

    *out = l.token_from_start(l.ident_type());
    return contains_letter;
}

def (l: *Lexer) lex_num(): Token {
    while is_num(l.current()) {
        l.advance();
    }

    // float
    if l.current() == '.' {
        l.advance();
        while is_num(l.current()) {
            l.advance();
        }

        return l.token_from_start(TokenType.Float);
    }

    return l.token_from_start(TokenType.Int);
}

def (l: *Lexer) lex_char_sequence(terminator: i32, ty: TokenType): Token {
    let error_start: string = null;
    let error_end: string = null;

    let should_continue = true;
    let escape_next = false;
    while !l.at_end() && should_continue {
        let curr = l.current();

        if curr == '\\' && !escape_next {
            escape_next = true;
        } else if curr == terminator {
            should_continue = escape_next;
            escape_next = false;
        } else {
            if escape_next {
                // every escapable character
                if curr != '0'
                    && curr != 'n'
                    && curr != 'r'
                    && curr != 't'
                    && curr != '\\'
                    && curr != '\''
                    && curr != '"' {

                    error_start = l.current_code_pos - 1;
                    error_end = l.current_code_pos + 1;
                }
            }

            escape_next = false;
        }

        l.advance();
    }

    if error_start != null {
        return l.create_token(TokenType.EscapeErr, error_start, error_end);
    }

    return l.create_token(ty, l.start + 1, l.current_code_pos - 1);
}

def (l: *Lexer) lex_string(): Token {
    return l.lex_char_sequence('"', TokenType.String);
}

def (l: *Lexer) lex_char(): Token {
    let char_seq = l.lex_char_sequence('\'', TokenType.Char);

    if char_seq.ty.is_err() {
        return char_seq;
    }

    let start = l.start + 1;
    let end = l.current_code_pos - 1;

    let len = l.current_code_pos - l.start;
    if len == 4 && *(l.start + 1) == '\\' {
        return l.create_token(TokenType.Char, start, end);
    }

    if len != 3 {
        return l.create_token(TokenType.CharErr, start, end);
    }

    return l.create_token(TokenType.Char, start, end);
}

def is_whitespace(c: i32): bool {
    return c == ' '
        || c == '\r'
        || c == '\n'
        || c == '\t'
        || c == '\0';
}

def (l: *Lexer) skip_until(ch: i32) {
    while !l.at_end() {
        l.advance();
        if l.current() == ch {
            return;
        }
    }
}

def (l: *Lexer) skip_whitespace() {
    let should_continue = true;
    while !l.at_end() && should_continue {
        let c = l.current();
        should_continue = is_whitespace(c);
        if should_continue {
            l.advance();
        }
    }
}

def (l: *Lexer) at_end(): bool {
    return *l.current_code_pos == '\0';
}

def (l: *Lexer) advance(): i32 {
    let read_bytes = l.current_pos();
    let c = 0;
    let len = read_char(l.current_code_pos, l.source.len - read_bytes, &c);

    l.current_code_pos += len;
    return c;
}

def (l: *Lexer) create_token(ty: TokenType, start: string, end: string): Token {
    return Token {
        ty: ty,
        span: span.create(start, end)
    };
}

def (l: *Lexer) create_token_from_span(ty: TokenType, start: string, span: span.Span): Token {
    return Token {
        ty: ty,
        span: span
    };
}

def (l: *Lexer) token_from_start(ty: TokenType): Token {
    return l.create_token(ty, l.start, l.current_code_pos);
}

def (l: *Lexer) get_next_token(): Token {
    if l.at_end() {
        let ptr = l.current_ptr();
        return Token {
            ty: TokenType.EOF,
            span: span.create(ptr, ptr)
        };
    }

    l.start = l.current_code_pos;
    let c = l.advance();

    if is_num(c) {
        return l.lex_num();
    }

    if is_letter(c) || c == '_' {
        let ident: Token = undefined;
        if l.lex_ident(&ident) || is_letter(c) || (ident.span.len() == 1 && c == '_') {
            return ident;
        }
        return l.token_from_start(TokenType.InvalidIdentifierErr);
    }

    if c == '"' {
        return l.lex_string();
    }

    if c == '\'' {
        return l.lex_char();
    }

    if c == ';' {
        return l.token_from_start(TokenType.Semi);
    }

    if c == ',' {
        return l.token_from_start(TokenType.Comma);
    }

    if c == ':' {
        return l.token_from_start(TokenType.Colon);
    }

    if c == '+' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.PlusEq);
        }
        return l.token_from_start(TokenType.Plus);
    }

    if c == '-' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.MinusEq);
        }
        return l.token_from_start(TokenType.Minus);
    }

    if c == '*' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.StarEq);
        }
        return l.token_from_start(TokenType.Star);
    }

    if c == '(' {
        return l.token_from_start(TokenType.LParen);
    }

    if c == ')' {
        return l.token_from_start(TokenType.RParen);
    }

    if c == '{' {
        return l.token_from_start(TokenType.LBrace);
    }

    if c == '}' {
        return l.token_from_start(TokenType.RBrace);
    }

    if c == '[' {
        return l.token_from_start(TokenType.LBracket);
    }

    if c == ']' {
        return l.token_from_start(TokenType.RBracket);
    }

    if c == '%' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.PercentEq);
        }
        return l.token_from_start(TokenType.Percent);
    }

    if c == '.' {
        if libc.memcmp(l.start as *void, "..." as *void, 3) == 0 {
            l.advance();
            l.advance();
            return l.token_from_start(TokenType.TripleDot);
        }

        return l.token_from_start(TokenType.Dot);
    }

    if c == '/' {
        if l.current() == '/' {
            l.skip_until('\n');
            return l.next_token();
        }
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.SlashEq);
        }
        return l.token_from_start(TokenType.Slash);
    }

    if c == '<' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.SmallerEq);
        }
        if l.current() == '<' {
            l.advance();
            return l.token_from_start(TokenType.SmallerSmaller);
        }
        return l.token_from_start(TokenType.Smaller);
    }

    if c == '>' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.GreaterEq);
        }
        if l.current() == '>' {
            l.advance();
            return l.token_from_start(TokenType.GreaterGreater);
        }
        return l.token_from_start(TokenType.Greater);
    }

    if c == '&' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.AmpersandEq);
        }
        if l.current() == '&' {
            l.advance();
            return l.token_from_start(TokenType.DoubleAmpersand);
        }
        return l.token_from_start(TokenType.Ampersand);
    }

    if c == '|' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.PipeEq);
        }
        if l.current() == '|' {
            l.advance();
            return l.token_from_start(TokenType.DoublePipe);
        }
        return l.token_from_start(TokenType.Pipe);
    }

    if c == '^' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.CaretEq);
        }
        return l.token_from_start(TokenType.Caret);
    }

    if c == '~' {
        return l.token_from_start(TokenType.Tilde);
    }

    if c == '=' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.DoubleEq);
        }
        return l.token_from_start(TokenType.Eq);
    }

    if c == '!' {
        if l.current() == '=' {
            l.advance();
            return l.token_from_start(TokenType.BangEq);
        }
        return l.token_from_start(TokenType.Bang);
    }


    return l.token_from_start(TokenType.UnknownErr);
}

// reads a single utf-8 char, stores it in 'out' and returns the number of bytes that were read
def read_char(s: *i8, s_len: usize, out: *i32): usize {
    let c_len: usize = 1;

    // see https://en.wikipedia.org/wiki/UTF-8#Examples
    if *s & 248 == 240 {
        c_len = 4;
    } else if *s & 240 == 224 {
        c_len = 3;
    } else if *s & 224 == 192 {
        c_len = 2;
    }

    if c_len > s_len {
        return s_len;
    }

    libc.memcpy(out as *void, s as *void, c_len);
    return c_len;
}
